{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UpTrain\n",
        "\n",
        "> UpTrain [[github](https://github.com/uptrain-ai/uptrain) || [website](https://uptrain.ai/) || [docs](https://docs.uptrain.ai/getting-started/introduction)] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them."
      ],
      "metadata": {
        "id": "J3dpzIoNzUes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UpTrain Callback Handler\n",
        "\n",
        "This notebook showcases the UpTrain callback handler seamlessly integrating into your pipeline, facilitating diverse evaluations. We have chosen a few evaluations that we deemed apt for evaluating the chains. These evaluations run automatically, with results displayed in the output. More details on UpTrain's evaluations can be found [here](https://github.com/uptrain-ai/uptrain?tab=readme-ov-file#pre-built-evaluations-we-offer-).\n",
        "\n",
        "Selected retievers from Langchain are highlighted for demonstration:\n",
        "\n",
        "### 1. **Vanilla RAG**:\n",
        "RAG plays a crucial role in retrieving context and generating responses. To ensure its performance and response quality, we conduct the following evaluations:\n",
        "\n",
        "- **[Context Relevance](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance)**: Determines if the context extracted from the query is relevant to the response.\n",
        "- **[Factual Accuracy](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy)**: Assesses if the LLM is hallcuinating or providing incorrect information.\n",
        "- **[Response Completeness](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness)**: Checks if the response contains all the information requested by the query.\n",
        "\n",
        "### 2. **Multi Query Generation**:\n",
        "MultiQueryRetriever creates multiple variants of a question having a similar meaning to the original question. Given the complexity, we include the previous evaluations and add:\n",
        "\n",
        "- **[Multi Query Accuracy](https://docs.uptrain.ai/predefined-evaluations/query-quality/multi-query-accuracy)**: Assures that the multi-queries generated mean the same as the original query.\n",
        "\n",
        "### 3. **Context Compression and Reranking**:\n",
        "Re-ranking involves reordering nodes based on relevance to the query and choosing top n nodes. Since the number of nodes can reduce once the re-ranking is complete, we perform the following evaluations:\n",
        "\n",
        "- **[Context Reranking](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-reranking)**: Checks if the order of re-ranked nodes is more relevant to the query than the original order.\n",
        "- **[Context Conciseness](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-conciseness)**: Examines whether the reduced number of nodes still provides all the required information.\n",
        "\n",
        "These evaluations collectively ensure the robustness and effectiveness of the RAG, MultiQueryRetriever, and the Reranking process in the chain."
      ],
      "metadata": {
        "id": "oI9WkotWs-kB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Dependencies**"
      ],
      "metadata": {
        "id": "AhYcP-qls3f4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c7f_3g8fzNvk"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community uptrain faiss-cpu flashrank"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "nzTTdPVdz8Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import FlashrankRerank\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_community.callbacks.uptrain_callback import UpTrainCallbackHandler\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_text_splitters import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")"
      ],
      "metadata": {
        "id": "8MsLTmORz2kt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the Data **"
      ],
      "metadata": {
        "id": "KOHtzJbU0kG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader=TextLoader(\"/content/Generative artificial intelligence .txt\")\n",
        "document=loader.load()"
      ],
      "metadata": {
        "id": "2oQWTNzmz2hT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "id": "7hMGVp_9sGIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert into Chunk**"
      ],
      "metadata": {
        "id": "ODw97tKKsFu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_spillter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=500)\n",
        "chunk=text_spillter.split_documents(documents=document)"
      ],
      "metadata": {
        "id": "iLcdS6C4z2Xm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk"
      ],
      "metadata": {
        "id": "xok0h097sNbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup the API Key**"
      ],
      "metadata": {
        "id": "jGqxLaArr2H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY='Enter your Openai Api Key'\n",
        "KEY_TYPE=\"openai\""
      ],
      "metadata": {
        "id": "pAcsJdUcbbLH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define the LLM model**"
      ],
      "metadata": {
        "id": "j7HQ3oPbq-dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "bEhfIwqUllbO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Store and create the retriever**"
      ],
      "metadata": {
        "id": "nCrGkAwbrOrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(chunk, embeddings)\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "PxAxvwQJz2Ll"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "UpTrain provides you with:\n",
        "1. Dashboards with advanced drill-down and filtering options\n",
        "1. Insights and common topics among failing cases\n",
        "1. Observability and real-time monitoring of production data\n",
        "1. Regression testing via seamless integration with your CI/CD pipelines\n",
        "\n",
        "You can choose between the following options for evaluating using UpTrain:\n",
        "### 1. **UpTrain's Open-Source Software (OSS)**:\n",
        "You can use the open-source evaluation service to evaluate your model. In this case, you will need to provie an OpenAI API key. UpTrain uses the GPT models to evaluate the responses generated by the LLM. You can get yours [here](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "In order to view your evaluations in the UpTrain dashboard, you will need to set it up by running the following commands in your terminal:\n",
        "\n",
        "```bash\n",
        "git clone https://github.com/uptrain-ai/uptrain\n",
        "cd uptrain\n",
        "bash run_uptrain.sh\n",
        "```\n",
        "\n",
        "This will start the UpTrain dashboard on your local machine. You can access it at `http://localhost:3000/dashboard`.\n",
        "\n",
        "Parameters:\n",
        "- key_type=\"openai\"\n",
        "- api_key=\"OPENAI_API_KEY\"\n",
        "- project_name=\"PROJECT_NAME\"\n",
        "\n",
        "\n",
        "### 2. **UpTrain Managed Service and Dashboards**:\n",
        "Alternatively, you can use UpTrain's managed service to evaluate your model. You can create a free UpTrain account [here](https://uptrain.ai/) and get free trial credits. If you want more trial credits, [book a call with the maintainers of UpTrain here](https://calendly.com/uptrain-sourabh/30min).\n",
        "\n",
        "The benefits of using the managed service are:\n",
        "1. No need to set up the UpTrain dashboard on your local machine.\n",
        "1. Access to many LLMs without needing their API keys.\n",
        "\n",
        "Once you perform the evaluations, you can view them in the UpTrain dashboard at `https://dashboard.uptrain.ai/dashboard`\n",
        "\n",
        "Parameters:\n",
        "- key_type=\"uptrain\"\n",
        "- api_key=\"UPTRAIN_API_KEY\"\n",
        "- project_name=\"PROJECT_NAME\"\n",
        "\n",
        "\n",
        "**Note:** The `project_name` will be the project name under which the evaluations performed will be shown in the UpTrain dashboard."
      ],
      "metadata": {
        "id": "KR46rq_PN0bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Vanilla RAG"
      ],
      "metadata": {
        "id": "pNUzLHNurhbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UpTrain callback handler will automatically capture the query, context and response once generated and will run the following three evaluations *(Graded from 0 to 1)* on the response:\n",
        "- **[Context Relevance](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance)**: Check if the context extractedfrom the query is relevant to the response.\n",
        "- **[Factual Accuracy](https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy)**: Check how factually accurate the response is.\n",
        "- **[Response Completeness](https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness)**: Check if the response contains all the information that the query is asking for."
      ],
      "metadata": {
        "id": "lzW27jYYrX1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the RAG prompt\n",
        "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "rag_prompt_text = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Create the chain\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt_text\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the uptrain callback handler\n",
        "uptrain_callback = UpTrainCallbackHandler(key_type=KEY_TYPE, api_key=API_KEY)\n",
        "config = {\"callbacks\": [uptrain_callback]}\n",
        "\n",
        "# Invoke the chain with a query\n",
        "query = \"What is use of Generative AI ?\"\n",
        "docs = chain.invoke(query, config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CQlntVnaPCu",
        "outputId": "0d9a2d3a-1152-4c39-d154-0d03aa0f8586"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-05-30 08:39:45.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:39:51.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:40:03.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:40:11.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Generative AI is used across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. It is capable of generating text, images, videos, or other data using generative models, often in response to prompts. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics. They are used in systems such as chatbots, text-to-image artificial intelligence image generation systems, and text-to-video AI generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Response: Generative AI is used across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. It is capable of generating text, images, videos, or other data using generative models, often in response to prompts. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics. They are used in systems such as chatbots, text-to-image artificial intelligence image generation systems, and text-to-video AI generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Relevance Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Context Relevance Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Factual Accuracy Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Factual Accuracy Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Completeness Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Response Completeness Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Multi Query Generation\n",
        "\n",
        "The **MultiQueryRetriever** is used to tackle the problem that the RAG pipeline might not return the best set of documents based on the query. It generates multiple queries that mean the same as the original query and then fetches documents for each.\n",
        "\n",
        "To evluate this retriever, UpTrain will run the following evaluation:\n",
        "- **[Multi Query Accuracy](https://docs.uptrain.ai/predefined-evaluations/query-quality/multi-query-accuracy)**: Checks if the multi-queries generated mean the same as the original query."
      ],
      "metadata": {
        "id": "oqaij_MQrlfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the retriever\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
        "\n",
        "# Create the uptrain callback\n",
        "uptrain_callback = UpTrainCallbackHandler(key_type=KEY_TYPE, api_key=API_KEY)\n",
        "config = {\"callbacks\": [uptrain_callback]}\n",
        "\n",
        "# Create the RAG prompt\n",
        "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "rag_prompt_text = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": multi_query_retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt_text\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Invoke the chain with a query\n",
        "question = \"What is use of Generative AI ?\"\n",
        "docs = chain.invoke(question, config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1O_9ZgKz15c",
        "outputId": "19d0d2bd-f5cb-452e-fdfa-f8c87cdfa551"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2024-05-30 08:41:33.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:41:37.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi Queries:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Multi Queries:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - How is Generative AI applied in various fields?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:  - How is Generative AI applied in various fields?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - What are the practical applications of Generative AI?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:  - What are the practical applications of Generative AI?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Can you explain the purpose and functionality of Generative AI?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:  - Can you explain the purpose and functionality of Generative AI?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi Query Accuracy Score: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Multi Query Accuracy Score: 0.5\n",
            "\u001b[32m2024-05-30 08:41:46.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:41:52.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:42:05.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:42:13.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Generative AI is used across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. It is capable of generating text, images, videos, or other data using generative models, often in response to prompts. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics. This technology has been used in chatbots, text-to-image artificial intelligence image generation systems, and text-to-video AI generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Response: Generative AI is used across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. It is capable of generating text, images, videos, or other data using generative models, often in response to prompts. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics. This technology has been used in chatbots, text-to-image artificial intelligence image generation systems, and text-to-video AI generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Relevance Score: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Context Relevance Score: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Factual Accuracy Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Factual Accuracy Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Completeness Score: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Response Completeness Score: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Context Compression and Reranking\n",
        "\n",
        "The reranking process involves reordering nodes based on relevance to the query and choosing the top n nodes. Since the number of nodes can reduce once the reranking is complete, we perform the following evaluations:\n",
        "- **[Context Reranking](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-reranking)**: Check if the order of re-ranked nodes is more relevant to the query than the original order.\n",
        "- **[Context Conciseness](https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-conciseness)**: Check if the reduced number of nodes still provides all the required information."
      ],
      "metadata": {
        "id": "Dm6d4-ThrrOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the retriever\n",
        "compressor = FlashrankRerank()\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "# Create the chain\n",
        "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
        "\n",
        "# Create the uptrain callback\n",
        "uptrain_callback = UpTrainCallbackHandler(key_type=KEY_TYPE, api_key=API_KEY)\n",
        "config = {\"callbacks\": [uptrain_callback]}\n",
        "\n",
        "# Invoke the chain with a query\n",
        "query = \"What is use of Generative AI ?\"\n",
        "result = chain.invoke(query, config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZnUYHpBqWOW",
        "outputId": "184cde7f-c987-4011-d1ac-2c5f5f62e4aa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:01<00:00, 76.7MiB/s]\n",
            "\u001b[32m2024-05-30 08:42:50.870\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:42:53.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:42:57.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Conciseness Score: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Context Conciseness Score: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Reranking Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Context Reranking Score: 0.0\n",
            "\u001b[32m2024-05-30 08:43:03.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:43:09.887\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:43:21.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.remote\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m437\u001b[0m - \u001b[1mSending evaluation request for rows 0 to <100 to the Uptrain server\u001b[0m\n",
            "\u001b[32m2024-05-30 08:43:27.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n",
            "Question: What is use of Generative AI ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Generative AI has uses across a wide range of industries. It can be used in software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. It is capable of generating text, images, videos, or other data using generative models, often in response to prompts. For example, it can be used to create high-quality artificial intelligence art from natural language prompts. It can also be used to develop chatbots, text-to-image artificial intelligence image generation systems, and text-to-video AI generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Response: Generative AI has uses across a wide range of industries. It can be used in software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. It is capable of generating text, images, videos, or other data using generative models, often in response to prompts. For example, it can be used to create high-quality artificial intelligence art from natural language prompts. It can also be used to develop chatbots, text-to-image artificial intelligence image generation systems, and text-to-video AI generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Relevance Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Context Relevance Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Factual Accuracy Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Factual Accuracy Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Completeness Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_community.callbacks.uptrain_callback:Response Completeness Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8suc7twBz1xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPZe-Q7Xz1rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wA3is9mfz1oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZS9Vx1UMz1j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwmN7EFOz1f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yLxwXlHwz1av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1KN2k0Wz1XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "712sp04pz1Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sOq5tDoez1PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtafwDlNz1L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zI5GfZAGz1IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hYSsMUItz1E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYmYYRi7z1BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lv2p6t8lz09T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbM8Ii6Ez06A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eoCU9g5Oz02K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s1f7M3tyz0ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9jczM4q4z0u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rG6a-uYMz0rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ELVoXMpDz0nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "se-R75Htz0kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-p52wGeWz0g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vwz-ntcZz0de"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}