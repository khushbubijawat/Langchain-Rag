{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxUoR1Nb0kH0"
      },
      "source": [
        "# **Install the required the libaries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GAr_8Wnfxvr"
      },
      "outputs": [],
      "source": [
        "%pip install llama_index ftfy regex tqdm\n",
        "%pip install -U openai-whisper\n",
        "%pip install git+https://github.com/openai/CLIP.git\n",
        "%pip install torch torchvision\n",
        "%pip install matplotlib scikit-image\n",
        "%pip install lancedb\n",
        "%pip install moviepy\n",
        "%pip install pytube\n",
        "%pip install pydub\n",
        "%pip install SpeechRecognition\n",
        "%pip install ffmpeg-python\n",
        "%pip install soundfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dl6cuLQ01tg"
      },
      "source": [
        "# **Import the required Libaries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTpl7HnDVDzG"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "from pathlib import Path\n",
        "import speech_recognition as sr\n",
        "from pytube import YouTube\n",
        "from pprint import pprint\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4fvbB2TVNr3"
      },
      "outputs": [],
      "source": [
        "video_url=\"https://youtu.be/Xr8QZHQm8PA?si=uhh0r0pGb6ugr4Rn\"\n",
        "output_video_path = \"/content/video_data/\"\n",
        "output_folder = \"/content/mixed_data/\"\n",
        "output_audio_path = \"/content/data/output_audio.wav\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF5dr4VbVNoN"
      },
      "outputs": [],
      "source": [
        "filepath=output_video_path + \"input_vid.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKGoRdNHVNk5"
      },
      "outputs": [],
      "source": [
        "filepath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc2Deshd1rsR"
      },
      "source": [
        "# **Processing on video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSxXiWN4VNen"
      },
      "outputs": [],
      "source": [
        "def plot_images(images_path):\n",
        "  images_shown = 0\n",
        "  plt.figure(figsize=(16, 9))\n",
        "  for img_path in images_path:\n",
        "        if os.path.isfile(img_path):\n",
        "            image = Image.open(img_path)\n",
        "\n",
        "            plt.subplot(2, 3, images_shown + 1)\n",
        "            plt.imshow(image)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "\n",
        "            images_shown += 1\n",
        "            if images_shown >= 5:\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y4ZS9vGVNbW"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube\n",
        "def download_video(url,output_path):\n",
        "  yt = YouTube(url)\n",
        "  metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n",
        "  yt.streams.get_highest_resolution().download(\n",
        "        output_path=output_path, filename=\"input_vid.mp4\"\n",
        "    )\n",
        "  return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWJ9zEZfVNX3"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "def video_to_images(video_path,output_flder):\n",
        "  clip=VideoFileClip(video_path)\n",
        "  clip.write_images_sequence(\n",
        "      os.path.join(output_folder,\"frame%04d.jpg\"),fps=0.2\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Aovj3XVNUk"
      },
      "outputs": [],
      "source": [
        "def video_to_audio(video_path,output_audio_path):\n",
        "  clip=VideoFileClip(video_path)\n",
        "  audio=clip.audio\n",
        "  audio.write_audiofile(output_audio_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03NGmttyVDvt"
      },
      "outputs": [],
      "source": [
        "def audio_to_text(audio_path):\n",
        "  recognizer=sr.Recognizer()\n",
        "  audio=sr.AudioFile(audio_path)\n",
        "\n",
        "  with audio as source:\n",
        "    audio_data=recognizer.record(source)\n",
        "\n",
        "    try:\n",
        "\n",
        "      #recognize the speech\n",
        "      text = recognizer.recognize_whisper(audio_data)\n",
        "\n",
        "    except sr.UnknownValueError:\n",
        "      print(\"Speech recognition could not understand the audio.\")\n",
        "  return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hgKTqXwVDsa"
      },
      "outputs": [],
      "source": [
        "video_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMVddjgtVDpF"
      },
      "outputs": [],
      "source": [
        "output_video_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47RplBBKVDlz"
      },
      "outputs": [],
      "source": [
        "metadata_vid = download_video(video_url, output_video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8vr4sewVDiM"
      },
      "outputs": [],
      "source": [
        "metadata_vid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdKlGUXPVDeW"
      },
      "outputs": [],
      "source": [
        "!mkdir mixed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUVnAcFK6QuN"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-xKeuCcVDa_"
      },
      "outputs": [],
      "source": [
        "filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTwzkG0Qfj6k"
      },
      "outputs": [],
      "source": [
        "output_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LSdu6xwfj3G"
      },
      "outputs": [],
      "source": [
        "video_to_images(filepath,output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suVYjbERfjza"
      },
      "outputs": [],
      "source": [
        "filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zICgO_-LfjwS"
      },
      "outputs": [],
      "source": [
        "output_audio_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXlkUc7ofjs-"
      },
      "outputs": [],
      "source": [
        "video_to_audio(filepath,output_audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsPI7d8sfjpc"
      },
      "outputs": [],
      "source": [
        "output_audio_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pg77eerJfjmL",
        "outputId": "d6090bee-383d-4002-a14d-6a6be1e429a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 153MiB/s]\n"
          ]
        }
      ],
      "source": [
        "text_data=audio_to_text(output_audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phaKB12mfjjC"
      },
      "outputs": [],
      "source": [
        "text_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_EKRSjUn0yy"
      },
      "outputs": [],
      "source": [
        "path=\"/content/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "etegIroVhgO5",
        "outputId": "edfca51e-2f47-440e-98c4-4930e6890f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text data saved to file\n"
          ]
        }
      ],
      "source": [
        "with open( path+ \"text.txt\", \"w\") as file:\n",
        "        file.write(text_data)\n",
        "print(\"Text data saved to file\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbH1IgL3oN-S"
      },
      "outputs": [],
      "source": [
        "os.remove(output_audio_path)\n",
        "print(\"Audio file removed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af0oY5pJfjZ0"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain  vertexai langchain_community chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnwov4i2-h0w"
      },
      "source": [
        "# **Configure the Vertex AI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64Fj7EAN-gV0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --user google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IemKqcMmf-Xy"
      },
      "outputs": [],
      "source": [
        "import getpass, os, requests\n",
        "\n",
        "if \"GCP_PROJECT_ID\" not in os.environ:\n",
        "  os.environ[\"GCP_PROJECT_ID\"] = getpass.getpass(\"Provide your GCP Project ID\")\n",
        "\n",
        "if \"LOCATION\" not in os.environ:\n",
        "  os.environ[\"LOCATION\"] = getpass.getpass(\"Provide your GCP Location\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X12gCKIMgUpa"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project {os.getenv(\"GCP_PROJECT_ID\")}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICP9SwU2gUl2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1LBa72KgUia"
      },
      "outputs": [],
      "source": [
        "!gcloud auth list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TryCAmtPgkB3"
      },
      "outputs": [],
      "source": [
        "# Define project information\n",
        "PROJECT_ID=os.getenv(\"GCP_PROJECT_ID\")\n",
        "LOCATION=os.getenv(\"LOCATION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gUJF4Bpgj-g"
      },
      "outputs": [],
      "source": [
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjc1rmCxfFk8"
      },
      "source": [
        "# **load the text documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wnJBizAfEl0"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"/content/data/text.txt\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIxVigq9cSd2"
      },
      "source": [
        "# **text data convert into chunk**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQQUzfo2cR1q"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 500)\n",
        "document = text_splitter.split_documents(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-BEeFHFgpW0"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.llms import VertexAI\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from vertexai.preview.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    HarmCategory,\n",
        "    HarmBlockThreshold,\n",
        "    Image,\n",
        "    Part\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8nU5MVc2sHi"
      },
      "source": [
        "# **Summarizes the text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ozcOADlgvTz"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
        "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "model = VertexAI(\n",
        "        temperature=0, model_name=\"gemini-pro\")\n",
        "\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im7Y_zTAi1rx"
      },
      "outputs": [],
      "source": [
        "print(type(text_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNF-hGQ4i8SE"
      },
      "outputs": [],
      "source": [
        "# Apply to texts\n",
        "texts = [i.text for i in document]\n",
        "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2LGp-vZjJY8"
      },
      "outputs": [],
      "source": [
        "text_summaries[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyyeQSeEjcKE"
      },
      "outputs": [],
      "source": [
        "path=\"/content/mixed_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kekXby3CDn"
      },
      "source": [
        "# **Summarizes the Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OcucN0bSkcg"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import base64\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.messages import HumanMessage, SystemMessage\n",
        "\n",
        "def encode_image(image_path):\n",
        "    ''' Getting the base64 string '''\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def image_summarize(img_base64,prompt):\n",
        "    ''' Image summary '''\n",
        "    chat = ChatVertexAI(model_name=\"gemini-pro-vision\", max_output_tokens=1024)\n",
        "    msg = chat.invoke(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\"type\": \"text\", \"text\":prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
        "                        },\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return msg.content\n",
        "\n",
        "# Store base64 encoded images\n",
        "img_base64_list = []\n",
        "\n",
        "# Store image summaries\n",
        "image_summaries = []\n",
        "# Prompt\n",
        "prompt = \"Describe the image in detail. Be specific about graphs, such as bar plots.\"\n",
        "\n",
        "# Read images, encode to base64 strings\n",
        "for img_file in sorted(os.listdir(path)):\n",
        "    if img_file.endswith('.jpg'):\n",
        "        img_path = os.path.join(path, img_file)\n",
        "        base64_image = encode_image(img_path)\n",
        "        img_base64_list.append(base64_image)\n",
        "        image_summaries.append(image_summarize(base64_image,prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzMwKoZFSln5"
      },
      "outputs": [],
      "source": [
        "image_summaries[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36L527Gz6ABd"
      },
      "source": [
        "# **Store the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeDS52178D2d"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.vision_models import MultiModalEmbeddingModel, Image\n",
        "embedding = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snC5P1Pc0M_P"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.schema.document import Document\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"multi_modal_rag\",\n",
        "                     embedding_function=embedding)\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMxbEyEw0M7y"
      },
      "outputs": [],
      "source": [
        "# Add texts\n",
        "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
        "summary_texts = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(text_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_texts)\n",
        "retriever.docstore.mset(list(zip(doc_ids, texts)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA4K8-iq6skA"
      },
      "outputs": [],
      "source": [
        "len(doc_ids), len(summary_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckIoac3k0M4h"
      },
      "outputs": [],
      "source": [
        "# Add image summaries\n",
        "img_ids = [str(uuid.uuid4()) for _ in img_base64_list]\n",
        "summary_img = [\n",
        "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
        "    for i, s in enumerate(image_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_img)\n",
        "retriever.docstore.mset(list(zip(img_ids, img_base64_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkfPJEve0M1j"
      },
      "outputs": [],
      "source": [
        "len(img_ids), len(summary_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgey_b_YFc0V"
      },
      "source": [
        "# **RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8cmwzR90Myb"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "def prompt_func(dict):\n",
        "    format_texts = \"\\n\".join(dict[\"context\"][\"texts\"])\n",
        "    return [\n",
        "        HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": f\"\"\"Answer the question based only on the following context, which can include text, tables, and the below image:\n",
        "Question: {dict[\"question\"]}\n",
        "\n",
        "Text :\n",
        "{format_texts}\n",
        "\"\"\"},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{dict['context']['images'][0]}\"}},\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
        "# RAG pipeline\n",
        "chain = (\n",
        "    {\"context\": retriever | RunnableLambda(split_image_text_types), \"question\": RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_func)\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNhlcuQPFQpO"
      },
      "outputs": [],
      "source": [
        "chain.invoke(\n",
        "    \"What is the change in wild fires from 1993 to 2022?\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
